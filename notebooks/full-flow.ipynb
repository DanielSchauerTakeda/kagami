{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a notebook for easily testing out components of the Azure Function without all the rigermarole of running a full Azure Function just to test out a specific piece of logic. It is not a replacement for unit tests and must be kept manually up-to-date with the Azure Function but it is convenient for experimenting with units of logic quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure.ai.documentintelligence in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 5)) (1.0.0b2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: azure-ai-textanalytics in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 6)) (5.3.0)\n",
      "Requirement already satisfied: azure.core in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 7)) (1.30.1)\n",
      "Requirement already satisfied: azure-functions in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 8)) (1.18.0)\n",
      "Requirement already satisfied: azure-functions-durable in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 9)) (1.2.9)\n",
      "Requirement already satisfied: langchain in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 10)) (0.1.11)\n",
      "Requirement already satisfied: langdetect in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 11)) (1.0.9)\n",
      "Requirement already satisfied: python-dotenv in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: semantic_kernel in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 13)) (0.9.1b1)\n",
      "Requirement already satisfied: azure.storage.blob in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 14)) (12.19.1)\n",
      "Requirement already satisfied: azure.ai.formrecognizer in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 15)) (3.3.2)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.ai.documentintelligence->-r ../src/requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.ai.documentintelligence->-r ../src/requirements.txt (line 5)) (4.10.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-ai-textanalytics->-r ../src/requirements.txt (line 6)) (1.1.28)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.core->-r ../src/requirements.txt (line 7)) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.core->-r ../src/requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: aiohttp>=3.6.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-functions-durable->-r ../src/requirements.txt (line 9)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-functions-durable->-r ../src/requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: furl>=2.1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-functions-durable->-r ../src/requirements.txt (line 9)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (2024.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (2.0.28)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.1.30)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (8.2.3)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (23.2.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (0.7.1)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (3.3.2)\n",
      "Requirement already satisfied: openai>=1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (1.13.3)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (23.6.21.0)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (2023.12.25)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.storage.blob->-r ../src/requirements.txt (line 14)) (42.0.5)\n",
      "Requirement already satisfied: msrest>=0.6.21 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.ai.formrecognizer->-r ../src/requirements.txt (line 15)) (0.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.9.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from cryptography>=2.1.4->azure.storage.blob->-r ../src/requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ../src/requirements.txt (line 10)) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ../src/requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from furl>=2.1.0->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain->-r ../src/requirements.txt (line 10)) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.29->langchain->-r ../src/requirements.txt (line 10)) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.29->langchain->-r ../src/requirements.txt (line 10)) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r ../src/requirements.txt (line 10)) (3.9.15)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from motor<4.0.0,>=3.3.2->semantic_kernel->-r ../src/requirements.txt (line 13)) (4.6.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from msrest>=0.6.21->azure.ai.formrecognizer->-r ../src/requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (4.66.2)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (3.7.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.7.1)\n",
      "Requirement already satisfied: parse in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (3.0.1)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.18.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain->-r ../src/requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain->-r ../src/requirements.txt (line 10)) (2.16.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r ../src/requirements.txt (line 10)) (3.0.3)\n",
      "Requirement already satisfied: pycparser in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure.storage.blob->-r ../src/requirements.txt (line 14)) (2.21)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.18.0)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic_kernel->-r ../src/requirements.txt (line 13)) (2.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure.ai.formrecognizer->-r ../src/requirements.txt (line 15)) (3.2.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.2.8)\n",
      "Requirement already satisfied: colorama in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r ../src/requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (2.1.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../src/requirements.txt\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langdetect import detect_langs\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Azure AI multi-service endpoint:  https://cajetzeraiservices.cognitiveservices.azure.com/\n",
      "Using Azure AI multi-service key:  ********************************\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ai_multiservice_endpoint = os.environ.get(\"AI_MULTISERVICE_ENDPOINT\")\n",
    "ai_multiservice_key = os.environ.get(\"AI_MULTISERVICE_KEY\")\n",
    "\n",
    "print(\"Using Azure AI multi-service endpoint: \", ai_multiservice_endpoint)\n",
    "print(\"Using Azure AI multi-service key: \", len(ai_multiservice_key) * \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "### validate_and_format_json\n",
    "Used in a similar manner to C#'s TryParse style methods where we test if the object is parsable as JSON and provide a tuple containing both a flag indiciating operational outcome and either the resulting JSON or a null value. It is possible we could reduce to just the JSON or ```null``` since the value of the boolean typically comes into play if you could potentially get a valid ```null``` value back for JSON but in this case that is functionally the same and also an unlikely outcome from Open AI which the is the intended use of this function - validating that Open AI produced a JSON response based on the prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_format_json(json_string, indent=4):\n",
    "    try:\n",
    "        # Attempt to parse the JSON string\n",
    "        parsed_json = json.loads(json_string)\n",
    "        # If successful, re-encode it with indentation for pretty printing\n",
    "        return True, json.dumps(parsed_json, indent=indent)\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, return an error message\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_text_to_boolean\n",
    "Similar to ```validate_and_format_json```, this is intended to facilitate validation of the Open AI output. In this case, we try to get a boolean response and if we get a valid one we provide it otherwise provide a null value. This could also be potentially simplified to just a boolean response but due to a poor understanding of downstream needs, we treat a failure to parse the boolean as different than a negative test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_to_boolean(text):\n",
    "    if text.lower() == \"true\":\n",
    "        return True\n",
    "    elif text.lower() == \"false\":\n",
    "        return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate_mode_response\n",
    "This is more complicated utility function that takes a list of comparable JSON objects and tries to find the mode value across all of the objects' properties. Currently, for simplicity, we assume a standard schema across. There are currently two 'bugs' I'm working through in this function\n",
    "1. List properties are not hashable and therefore can't be used as keys for the temporary dictionary of responses used to identify the mode response. I will likely need to flatten list responses and for each value in the list add them to the response\n",
    "2. The function does not currently handle empty JSON objects effectively\n",
    "I'm concerned that this will not be performant mostly due to a lack of familarity with Python code. I think this is a reasonable way to approach this problem though I could imagine someone coming up with a more creative and performant solve from a logic perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mode_response(list_of_json_objects):\n",
    "#     print(list_of_json_objects)\n",
    "\n",
    "#     if len(list_of_json_objects) == 0:\n",
    "\n",
    "#         return None\n",
    "\n",
    "#     # remove any None objects from the list\n",
    "#     list_of_json_objects = [val for val in list_of_json_objects if val is not None]\n",
    "\n",
    "#     first_entry = json.loads(list_of_json_objects[0])\n",
    "\n",
    "#     property_keys = first_entry.keys()\n",
    "\n",
    "#     mode_response = {}\n",
    "\n",
    "#     responses = {}\n",
    "\n",
    "#     for key in property_keys:\n",
    "\n",
    "#         for obj in list_of_json_objects:\n",
    "#             if isinstance(obj, list):\n",
    "#                 print(f\"{obj} is a list in {list_of_json_objects}\", list_of_json_objects)\n",
    "\n",
    "#             json_obj = json.loads(obj)\n",
    "\n",
    "#             value = json_obj[key]\n",
    "\n",
    "#             # todo: determine how to handle list values, for now skipping to prepare for demo\n",
    "\n",
    "#             if isinstance(value, list):\n",
    "\n",
    "#                 continue\n",
    "\n",
    "#             if json_obj[key] in responses:\n",
    "\n",
    "#                 responses[value] += 1\n",
    "\n",
    "#             else:\n",
    "\n",
    "#                 responses[value] = 1\n",
    "\n",
    "#         if len(responses) > 0:\n",
    "\n",
    "#             mode_response[key] = max(responses, key=responses.get)\n",
    "\n",
    "#         responses = {}\n",
    "\n",
    "#     return mode_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten_json\n",
    "try to test this out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(json_obj):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=\"\"):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + \"_\")\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + \"_\")\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(json_obj)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "I discuss chunking techniques at greater length in chunking-research.ipynb. Here I'm just trying to get the chunks available to leverage in testing the rest of the flow. *Notable changes* here is that I have turned it into a function and changed uri_path to file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(file_path):\n",
    "    ai_doc_intel_loader = AzureAIDocumentIntelligenceLoader(\n",
    "        file_path=file_path,\n",
    "        api_key=ai_multiservice_key,\n",
    "        api_endpoint=ai_multiservice_endpoint,\n",
    "        api_model=\"prebuilt-layout\",\n",
    "    )\n",
    "\n",
    "    docs = ai_doc_intel_loader.load()\n",
    "\n",
    "    # Split the document into chunks base on markdown headers.\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    docs_string = docs[0].page_content\n",
    "\n",
    "    splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "    chunks = []\n",
    "    \n",
    "    for split in splits:\n",
    "        chunks.append(split.page_content)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Introduction & Conclusion Chunks\n",
    "This is an area that could use more work and experimentation. You might be able to do some more tedious non-AI relatd coding around the markdown. I've tried a technique with mixed results where we summarize each chunk to 1 sentence, number the sentences by chunk index, and then have AOAI guess which sentences belong to the summary. I think that could work with more refined prompting. For now running with a more naive solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_introduction_chunks(chunks):\n",
    "    intro_chunks = []\n",
    "    intro_chunks.append(chunks[0])  \n",
    "    return intro_chunks\n",
    "\n",
    "def identify_conclusion_chunks(chunks):\n",
    "    conclusion_chunks = []\n",
    "    conclusion_chunks.append(chunks[-1])\n",
    "    return conclusion_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Semantic Kernel\n",
    "....\n",
    "- changed plugin directory so it borrows from plugin code actually in the project, could try to do this with some of the other code to which keeps it easier to maintain parity between notebook and function but then removes the benefit of getting to easily explain code side by side...\n",
    "\n",
    "todo: functionalize and return kernel\n",
    "\n",
    "```__file__``` not available in Jupyter notebook so using os.getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using aoai_deployment: gpt-35-turbo-16k-deployment\n",
      "Using aoai_endpoint: https://exp-aoai.openai.azure.com/\n",
      "Using aoai_key: **********************************\n"
     ]
    }
   ],
   "source": [
    "curr_directory = os.getcwd()\n",
    "plugins_directory = os.path.join(curr_directory, \"..\\\\src\\\\plugins\")\n",
    "\n",
    "# setup semantic kernel\n",
    "aoai_deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "aoai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "print(f\"Using aoai_deployment: {aoai_deployment}\")\n",
    "print(f\"Using aoai_endpoint: {aoai_endpoint}\")\n",
    "print(f\"Using aoai_key: {len(aoai_endpoint)*'*'}\")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "service = AzureChatCompletion(\n",
    "    service_id=service_id,\n",
    "    deployment_name=aoai_deployment,\n",
    "    endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    ")\n",
    "\n",
    "kernel.add_service(service)\n",
    "\n",
    "plugin_names = [\n",
    "    plugin\n",
    "    for plugin in os.listdir(plugins_directory)\n",
    "    if os.path.isdir(os.path.join(plugins_directory, plugin))\n",
    "]\n",
    "\n",
    "# for each plugin, add the plugin to the kernel\n",
    "try:\n",
    "    for plugin_name in plugin_names:\n",
    "        kernel.import_plugin_from_prompt_directory(plugins_directory, plugin_name)\n",
    "except ValueError as e:\n",
    "    logging.exception(f\"Plugin {plugin_name} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Document\n",
    "I believe this is possible through just evaluating the introduction and conclusion to save on cost but still get a relatively accurate result. Occasionally getting what I think are accurate results. For larger documents it might be that you can't join the chunks so this may need to be revised to call the plugin against each chunk and then find the mode of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def classify_study_type(chunks, kernel):\n",
    "    joined_chunks = f\"{os.linesep}\".join(chunks)\n",
    "    skresult = await kernel.invoke(\n",
    "        kernel.plugins[\"ClassificationPlugin\"][\"ClassifyStudyType\"],\n",
    "        sk.KernelArguments(input=joined_chunks,tense=\"present\"),\n",
    "    )\n",
    "    _, intro_study_type_classification = validate_and_format_json(\n",
    "        skresult.value[0].content, None\n",
    "    )\n",
    "    return intro_study_type_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Here we attempt to determine if the findings are both conclusive and significant, again I believe this can be done just using the introductory chunks. Similar to classification, if chunks are too large for larger documents we maybe need to perform this on a per chunk basis and then avg results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def are_findings_conclusive_and_signficant(chunks, kernel):\n",
    "    joined_chunks = f\"{os.linesep}\".join(chunks)\n",
    "    skresult = await kernel.invoke(\n",
    "        kernel.plugins[\"SummaryPlugin\"][\"AreStudyFindingsSignificant\"],\n",
    "        sk.KernelArguments(input=joined_chunks),\n",
    "    )\n",
    "    return parse_text_to_boolean(skresult.value[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction\n",
    "This is where things get busy. We have stakeholders, dates, and then a multi-entity extraction function that we are performing against all chunks to make sure we don't miss a possibility and then we find the mode of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mode_response(list_of_json_objects, printObjs=False):\n",
    "    if len(list_of_json_objects) == 0:\n",
    "        return None\n",
    "\n",
    "    # remove any None objects from the list\n",
    "    list_of_json_objects = [val for val in list_of_json_objects if val is not None]\n",
    "\n",
    "    if printObjs:\n",
    "        print(\"List of JSON Objects: \", list_of_json_objects)\n",
    "\n",
    "    first_entry = list_of_json_objects[0]\n",
    "    # hacky conversion of other types to dictionary for keys\n",
    "    schema = (\n",
    "        json.loads(first_entry)\n",
    "        if isinstance(first_entry, str)\n",
    "        else json.loads(json.dumps(first_entry))\n",
    "    )\n",
    "\n",
    "    if printObjs:\n",
    "        print(\"Schema: \", schema)\n",
    "\n",
    "    property_keys = schema.keys()\n",
    "\n",
    "    mode_response = {}\n",
    "    responses = {}\n",
    "\n",
    "    for key in property_keys:\n",
    "        if printObjs:\n",
    "            print(\"Beginning loop for key: \", key)\n",
    "        for obj in list_of_json_objects:\n",
    "            if printObjs:\n",
    "                print(\"Evaluating: \", obj)\n",
    "\n",
    "            # hacky conversion of other types to dictionary for keysl see above\n",
    "            json_obj = (\n",
    "                json.loads(obj) if isinstance(obj, str) else json.loads(json.dumps(obj))\n",
    "            )\n",
    "\n",
    "            flatten_json(json_obj)\n",
    "            if isinstance(obj, list):\n",
    "                print(\n",
    "                    f\"{obj} is a list in {list_of_json_objects}\", list_of_json_objects\n",
    "                )\n",
    "\n",
    "            value = json_obj[key]\n",
    "\n",
    "            # todo: determine how to handle list values, for now skipping to prepare for demo\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "\n",
    "            if json_obj[key] in responses:\n",
    "                responses[value] += 1\n",
    "            else:\n",
    "                responses[value] = 1\n",
    "\n",
    "        if len(responses) > 0:\n",
    "            mode_response[key] = max(responses, key=responses.get)\n",
    "        responses = {}\n",
    "    return mode_response\n",
    "\n",
    "\n",
    "# for lists like this ['[{\"a\": \"b\"}, {\"c\": \"d\"}]', '[{\"e\": \"f\"}, {\"g\": \"h\"}]'] -> [{\"a\": \"b\"}, {\"c\": \"d\"}, {\"e\": \"f\"}, {\"g\": \"h\"}]\n",
    "def flatten_list_of_list_strings(original_list):\n",
    "    list_of_lists = [json.loads(val) for val in original_list if val is not None]\n",
    "    return [val for sublist in list_of_lists for val in sublist]\n",
    "\n",
    "\n",
    "###\n",
    "async def use_extraction_function(chunk, function_name, kernel):\n",
    "    extract_entities_result = await kernel.invoke(\n",
    "        kernel.plugins[\"EntityExtraction\"][function_name],\n",
    "        sk.KernelArguments(input=chunk),\n",
    "    )\n",
    "    # print(extract_entities_result.value[0].content)\n",
    "    _, extracted_entities = validate_and_format_json(\n",
    "        extract_entities_result.value[0].content, None\n",
    "    )\n",
    "\n",
    "    return extracted_entities\n",
    "\n",
    "\n",
    "\n",
    "async def extract_entities(chunks, kernel):\n",
    "    extracted_entity_responses = []\n",
    "    extracted_stakeholders_responses = []\n",
    "    extracted_dates_responses = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        extract_entities = await use_extraction_function(\n",
    "            chunk, \"ExtractMultipleEntities\", kernel\n",
    "        )\n",
    "        extracted_entity_responses.append(extract_entities)\n",
    "        extracted_stakeholders = await use_extraction_function(\n",
    "            chunk, \"ExtractStakeholders\", kernel\n",
    "        )\n",
    "        extracted_stakeholders_responses.append(extracted_stakeholders)\n",
    "        extracted_dates = await use_extraction_function(\n",
    "            chunk, \"ExtractSignificantDates\", kernel\n",
    "        )\n",
    "        extracted_dates_responses.append(extracted_dates)\n",
    "\n",
    "\n",
    "    # mode entity extraction results across chunks\n",
    "\n",
    "    # multi entities\n",
    "    mode_extracted_entities_response = calculate_mode_response(\n",
    "        extracted_entity_responses\n",
    "    )\n",
    "\n",
    "    # stakeholders\n",
    "    extracted_stakeholders_responses = flatten_list_of_list_strings(\n",
    "        extracted_stakeholders_responses\n",
    "    )\n",
    "    mode_extracted_stakeholders_response = calculate_mode_response(\n",
    "        extracted_stakeholders_responses\n",
    "    )\n",
    "\n",
    "    # dates\n",
    "    extracted_dates_responses = flatten_list_of_list_strings(extracted_dates_responses)\n",
    "    mode_extracted_dates_response = calculate_mode_response(\n",
    "        extracted_dates_responses\n",
    "    )\n",
    "\n",
    "\n",
    "    deep_final_result = {\n",
    "        \"avg_extracted_entities_response\": mode_extracted_entities_response,\n",
    "        \"avg_extracted_stakeholders_response\": mode_extracted_stakeholders_response,\n",
    "        \"avg_extracted_dates_response\": mode_extracted_dates_response,\n",
    "    }\n",
    "\n",
    "\n",
    "    flat_final_result = flatten_json(deep_final_result)\n",
    "\n",
    "    return flat_final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying together the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating against document: c:\\Projects\\kagami\\notebooks\\..\\sample-docs\\2 - jbm-5-015.pdf\n",
      "Processing chunk 0\n",
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "{\n",
      "    \"entities_avg_extracted_entities_response_Duration\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_DrugOrCompound\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_RouteOfAdministration\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_InternalStudyNumber\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_ExternalStudyNumber\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_TestFacility\": \"\",\n",
      "    \"entities_avg_extracted_stakeholders_response_name\": \"methylprednisolone\",\n",
      "    \"entities_avg_extracted_stakeholders_response_contact\": \"\",\n",
      "    \"entities_avg_extracted_dates_response_date\": \"2014\",\n",
      "    \"languages_0_language\": \"en\",\n",
      "    \"languages_0_probability\": 0.9999948169844857,\n",
      "    \"are_findings_significant\": false,\n",
      "    \"study_type\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# todo: read document names from sample docs directory and then loop through\n",
    "\n",
    "curr_directory = os.getcwd()\n",
    "sample_docs_directory = os.path.join(curr_directory, \"..\\\\sample-docs\")\n",
    "dir_list = os.listdir(sample_docs_directory)\n",
    "\n",
    "file_path = os.path.join(sample_docs_directory, dir_list[0])\n",
    "print(f\"Operating against document: {file_path}\")\n",
    "\n",
    "chunks = chunk_document(file_path)\n",
    "intro_chunks = identify_introduction_chunks(chunks)\n",
    "conclusion_chunks = identify_conclusion_chunks(chunks)\n",
    "\n",
    "combined_intro_conclusion_chunks = intro_chunks + conclusion_chunks\n",
    "\n",
    "study_type = await classify_study_type(combined_intro_conclusion_chunks, kernel)\n",
    "\n",
    "are_findings_significant = await are_findings_conclusive_and_signficant(\n",
    "    combined_intro_conclusion_chunks, kernel\n",
    ")  # potentially just use conclusion chunks\n",
    "\n",
    "# right now, we are just using the first chunk to determine the language, depending on business case you could use all chunks in the event it is a multi-language document you'll be more likely to get a more accurate result\n",
    "languages_result = detect_langs(intro_chunks[0])\n",
    "languages = []\n",
    "for lang in languages_result:\n",
    "    languages.append({\"language\": lang.lang, \"probability\": lang.prob})\n",
    "\n",
    "entities = await extract_entities(chunks, kernel)\n",
    "\n",
    "deep_final_result = {\n",
    "    \"entities\": entities,\n",
    "    \"languages\": languages,\n",
    "    \"are_findings_significant\": are_findings_significant,\n",
    "    \"study_type\": study_type,\n",
    "}\n",
    "\n",
    "flat_final_result = flatten_json(deep_final_result)\n",
    "\n",
    "print(json.dumps(flat_final_result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check_for_handwritten_signature\n",
    "testing out the ability to detect a handwritten signature in the chunk of a document using document intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating against document: c:\\GitHub\\kagami\\notebooks\\..\\sample-docs\\DLM_Window_Quote_Jan21_signed.pdf\n",
      "has_handwritten_sig = True\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "curr_directory = os.getcwd()\n",
    "sample_docs_directory = os.path.join(curr_directory, \"..\\\\sample-docs\")\n",
    "dir_list = os.listdir(sample_docs_directory)\n",
    "\n",
    "file_path = os.path.join(sample_docs_directory, dir_list[0])\n",
    "print(f\"Operating against document: {file_path}\")\n",
    "\n",
    "# Replace 'ai_multiservice_key' and 'ai_multiservice_endpoint' with your actual credentials\n",
    "\n",
    "def check_for_handwritten_style(file_path):\n",
    "    # Extract the text chunk from the arguments\n",
    "    chunks = chunk_document(file_path)\n",
    "\n",
    "    # Use the Document Intelligence API to analyze the text in the last chunk\n",
    " \n",
    "    text_chunk = chunks[-1]\n",
    "    # api_key = os.environ.get(\"AZURE_FORM_RECOGNIZER_API_KEY\")\n",
    "    # endpoint = os.environ.get(\"AZURE_FORM_RECOGNIZER_ENDPOINT\")\n",
    "    credential = AzureKeyCredential(ai_multiservice_key)\n",
    "    client = DocumentIntelligenceClient(endpoint=ai_multiservice_endpoint, credential=credential)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        poller = client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\"\n",
    "        )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    if result.styles and any([style.is_handwritten for style in result.styles]):\n",
    "        return True\n",
    "    else: return False\n",
    "        # result = client.begin_analyze_document(text_chunk).result()\n",
    "\n",
    "    # Check if the text is handwritten style\n",
    "    # is_handwritten_style = result.isHandwritten()\n",
    "    # return is_handwritten_style\n",
    "\n",
    "has_handwritten_sig = check_for_handwritten_style(file_path)\n",
    "print(\"has_handwritten_sig = \" + str(has_handwritten_sig))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
